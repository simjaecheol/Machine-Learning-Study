{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d44fd004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f175e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGAN:\n",
    "    def __init__(self):\n",
    "        (self.x_train, _), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "        self.init_dim = 7\n",
    "        self.strides = (2, 2, 1)\n",
    "        self.data_shape = self.x_train.shape + (1,)\n",
    "\n",
    "        # get basic inputs\n",
    "        self.batch_size =  128\n",
    "        self.noise_dim =  128\n",
    "        self.total_epoch =  100\n",
    "        self.critic_step =  1\n",
    "        self.visualize = True\n",
    "        self.out_path = os.getcwd()\n",
    "\n",
    "        # storage for the objectives\n",
    "        self.batch_num = int(self.data_shape[0] / self.batch_size) + (self.data_shape[0] % self.batch_size != 0)\n",
    "        self.d_obj = np.zeros([self.batch_num, self.total_epoch, self.critic_step])\n",
    "        self.g_obj = np.zeros([self.batch_num, self.total_epoch])\n",
    "\n",
    "        # set regularization parameters\n",
    "        self.grad_penalty =  10.0\n",
    "        self.perturb_factor =  1.0\n",
    "\n",
    "        # normalize dataset\n",
    "        self.x_train = self.x_train.reshape(self.data_shape).astype('float32')\n",
    "        self.x_train = (self.x_train - 127.5) / 127.5  # Normalize RGB to [-1, 1]\n",
    "        self.x_train = \\\n",
    "            tf.data.Dataset.from_tensor_slices(self.x_train).shuffle(self.data_shape[0]).batch(self.batch_size)\n",
    "\n",
    "        # setup optimizers\n",
    "        self.D_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4,\n",
    "                                                    beta_1=0.5,\n",
    "                                                    beta_2= 0.999,\n",
    "                                                    epsilon=1e-7,\n",
    "                                                    amsgrad= False)\n",
    "        self.G_optimizer = tf.keras.optimizers.Adam(learning_rate= 5e-5,\n",
    "                                                    beta_1=0.2,\n",
    "                                                    beta_2= 0.999,\n",
    "                                                    epsilon= 1e-7,\n",
    "                                                    amsgrad=False)\n",
    "\n",
    "        # setup models\n",
    "        self.G = self.set_generator()\n",
    "        self.D = self.set_discriminator()\n",
    "\n",
    "    def set_generator(self):\n",
    "        g = tf.keras.Sequential()\n",
    "        g.add(layers.Dense(self.init_dim * self.init_dim * 256, use_bias=False, input_shape=(self.noise_dim,)))\n",
    "        g.add(layers.BatchNormalization())\n",
    "        g.add(layers.LeakyReLU())\n",
    "        g.add(layers.Reshape((self.init_dim, self.init_dim, 256)))\n",
    "\n",
    "        g.add(layers.Conv2DTranspose(128, 5, strides=self.strides[0], padding='same', use_bias=False))\n",
    "        g.add(layers.BatchNormalization())\n",
    "        g.add(layers.LeakyReLU())\n",
    "\n",
    "        g.add(layers.Conv2DTranspose(64, 5, strides=self.strides[1], padding='same', use_bias=False))\n",
    "        g.add(layers.BatchNormalization())\n",
    "        g.add(layers.LeakyReLU())\n",
    "\n",
    "        g.add(layers.Conv2DTranspose(32, 5, strides=self.strides[2], padding='same', use_bias=False))\n",
    "        g.add(layers.BatchNormalization())\n",
    "        g.add(layers.LeakyReLU())\n",
    "\n",
    "        g.add(layers.Conv2DTranspose(self.data_shape[3], 5, strides=self.strides[2],\n",
    "                                     padding='same', use_bias=False, activation='tanh'))\n",
    "\n",
    "        return g\n",
    "\n",
    "    def set_discriminator(self):\n",
    "        d = tf.keras.Sequential()\n",
    "        d.add(layers.Conv2D(32, kernel_size=5, strides=2, padding='same', input_shape=self.data_shape[1:]))\n",
    "        d.add(layers.LeakyReLU())\n",
    "\n",
    "        d.add(layers.Conv2D(64, kernel_size=5, strides=2, padding='same'))\n",
    "        d.add(layers.LayerNormalization())\n",
    "        d.add(layers.LeakyReLU())\n",
    "\n",
    "        d.add(layers.Conv2D(128, kernel_size=5, strides=2, padding='same'))\n",
    "        d.add(layers.LayerNormalization())\n",
    "        d.add(layers.LeakyReLU())\n",
    "\n",
    "        d.add(layers.Flatten())\n",
    "        d.add(layers.Dense(1))\n",
    "\n",
    "        return d\n",
    "\n",
    "    @tf.function\n",
    "    def lipschitz_penalty(self, x, x_hat):\n",
    "        # DRAGAN-like sampling scheme\n",
    "        x_join = tf.concat([x, x_hat], axis=0)\n",
    "        _, batch_var = tf.nn.moments(x_join, axes=[0, 1, 2, 3])\n",
    "        delta = tf.random.normal(x_join.shape, stddev=self.perturb_factor * tf.sqrt(batch_var))\n",
    "        x_tilde = x_join + delta\n",
    "\n",
    "        # compute gradient penalty\n",
    "        with tf.GradientTape() as D_tape:\n",
    "            D_tape.watch(x_tilde)\n",
    "            y_tilde = self.D(x_tilde)\n",
    "        d_grad = D_tape.gradient(y_tilde, x_tilde)\n",
    "        grad_norm = tf.sqrt(tf.reduce_sum(tf.square(d_grad), axis=[1, 2, 3]))\n",
    "\n",
    "        return tf.reduce_mean(tf.square(tf.maximum(0.0, grad_norm - 1.0)))\n",
    "\n",
    "    @tf.function\n",
    "    def train_discriminator(self, x_batch):\n",
    "        with tf.GradientTape() as D_tape:\n",
    "            # sample data\n",
    "            x_gen = self.G(tf.random.uniform([x_batch.shape[0], self.noise_dim]), training=True)\n",
    "\n",
    "            # scoring with the discriminator\n",
    "            y_real = self.D(x_batch, training=True)\n",
    "            y_gen = self.D(x_gen, training=True)\n",
    "\n",
    "            # compute the objective\n",
    "            d_obj = tf.math.reduce_mean(y_gen) - tf.math.reduce_mean(y_real)\n",
    "            d_obj_pen = d_obj + self.grad_penalty * self.lipschitz_penalty(x_batch, x_gen)\n",
    "        # update the discriminator\n",
    "        d_grad = D_tape.gradient(d_obj_pen, self.D.trainable_variables)\n",
    "        self.D_optimizer.apply_gradients(zip(d_grad, self.D.trainable_variables))\n",
    "\n",
    "        return d_obj\n",
    "\n",
    "    @tf.function\n",
    "    def train_generator(self, x_batch_size):\n",
    "        with tf.GradientTape() as G_tape:\n",
    "            x_gen = self.G(tf.random.uniform([x_batch_size, self.noise_dim]), training=True)\n",
    "            y_gen = self.D(x_gen, training=True)\n",
    "\n",
    "            # compute the objective\n",
    "            g_obj = -tf.math.reduce_mean(y_gen)\n",
    "        # update the generator\n",
    "        g_grad = G_tape.gradient(g_obj, self.G.trainable_variables)\n",
    "        self.G_optimizer.apply_gradients(zip(g_grad, self.G.trainable_variables))\n",
    "\n",
    "        return g_obj\n",
    "\n",
    "    def train(self):\n",
    "        vis_seed = None\n",
    "        if self.visualize:\n",
    "            # Seed for checking training progress\n",
    "            vis_seed = tf.random.uniform([16, self.noise_dim])\n",
    "\n",
    "        # Record current time and start training\n",
    "        print(\"Training...\")\n",
    "        ts_start = tf.timestamp()\n",
    "        for t in range(self.total_epoch):\n",
    "            batch_id = 0\n",
    "            for b in self.x_train:\n",
    "                for k in range(self.critic_step):\n",
    "                    self.d_obj[batch_id, t, k] = self.train_discriminator(b)\n",
    "                self.g_obj[batch_id, t] = self.train_generator(b.shape[0])\n",
    "                batch_id += 1\n",
    "\n",
    "            # Print time\n",
    "            print(\"Time used for epoch {} are {:0.2f} seconds.\".format(t + 1, tf.timestamp() - ts_start))\n",
    "\n",
    "            # Check current generator\n",
    "            if self.visualize:\n",
    "                vis_gen = self.G(vis_seed, training=False)\n",
    "                fig = plt.figure(figsize=(4, 4))\n",
    "                plt.suptitle('Epoch: {:03d}'.format(t + 1))\n",
    "                for i in range(vis_gen.shape[0]):\n",
    "                    plt.subplot(4, 4, i + 1)\n",
    "                    if self.data_shape[3] == 1:\n",
    "                        plt.imshow(vis_gen[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "                    else:\n",
    "                        plt.imshow((vis_gen[i, :, :] + 1) / 2)\n",
    "                    plt.axis('off')\n",
    "                plt.savefig(os.path.join(self.out_path, \"WGAN_{}_Epoch_{:03d}.png\".format(\"MNIST\", t + 1)))\n",
    "                plt.clf()\n",
    "                plt.close(fig)\n",
    "        print(\"Done! {:0.2f} seconds have passed.\".format(tf.timestamp() - ts_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "03964325",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WGAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab2a2e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Time used for epoch 1 are 1064.78 seconds.\n",
      "Time used for epoch 2 are 2124.62 seconds.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19128/1676823408.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19128/323666585.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    151\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0md_obj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_discriminator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mg_obj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m                 \u001b[0mbatch_id\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 910\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    940\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 942\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    943\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3128\u001b[0m       (graph_function,\n\u001b[0;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3130\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3131\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1957\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1958\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1959\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1960\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    599\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     60\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb51eabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_seed = tf.random.uniform([100, model.noise_dim])\n",
    "vis_gen = model.G(vis_seed, training=False)\n",
    "if model_param.dataset == \"MNIST\":\n",
    "    plt.figure(figsize=(3.45, 3.45))\n",
    "else:\n",
    "    plt.figure(figsize=(3.85, 3.85))\n",
    "for i in range(vis_gen.shape[0]):\n",
    "    x_pos = i % 10\n",
    "    y_pos = int(i / 10)\n",
    "    if model_param.dataset == \"MNIST\":\n",
    "        plt.figimage(vis_gen[i, :, :, 0] * 127.5 + 127.5,\n",
    "                     10 + x_pos * (28 + 5), 10 + y_pos * (28 + 5), cmap='gray')\n",
    "    else:\n",
    "        plt.figimage((vis_gen[i, :, :] + 1) / 2,\n",
    "                     10 + x_pos * (32 + 5), 10 + y_pos * (32 + 5))\n",
    "    plt.axis('off')\n",
    "plt.savefig(os.path.join(model_param.output,\n",
    "                         \"{}_{}_Example.png\".format(model_param.model, model_param.dataset)))\n",
    "\n",
    "# plot median value of the objective functions\n",
    "plt.figure()\n",
    "plt.title(\"Objective Functions of {} (Dataset: {})\".format(model_param.model, model_param.dataset))\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Median Value\")\n",
    "plt.plot(range(1, 1 + model_param.total_epoch), np.median(model.d_obj, axis=[-0, -1]))\n",
    "plt.plot(range(1, 1 + model_param.total_epoch), np.median(model.g_obj, axis=[-0]))\n",
    "plt.legend(['Discriminator', 'Generator'])\n",
    "plt.savefig(os.path.join(model_param.output,\n",
    "                         \"{}_{}_Objective.png\".format(model_param.model, model_param.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e925a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
